# Azure Data Engineering Project
An end-to-end ETL pipeline implementing **bronze-silver-gold**  architecture on Azure

![Architecture2](docs/architecture-2.png "Project Architecture 2")

## ğŸ“Œ Architecture Overview
1. **Extract**: GitHub â†’ Azure SQL DB (via Data Factory)
2. **Bronze**: Raw data landing in Azure Data Lake
3. **Silver**: Databricks transformations (cleaning, validation)
4. **Gold**: Delta Lake star schema with SCD Type 1 UPSERT
5. **Visualization**: Power BI with live Databricks connection

## ğŸ› ï¸ Technologies Used
| Technology | Purpose |
|------------|---------|
| Azure Data Factory | Orchestration & data movement |
| Azure Databricks | Data transformation (PySpark/SQL) |
| Delta Lake | ACID transactions in Gold layer |
| Azure SQL DB | Initial data staging |
| Azure Blob Storage | Data Lake implementation |
| Power BI | Analytics & visualization |


## ğŸ“‚ Repository Structure
```
azure-databrics-ETL-Pipeline/
â”œâ”€â”€ Incremental_pipeline/   # Data Factory pipelines & JSON definitions
â”œâ”€â”€ notebooks/              # Notebooks for each layer
â”‚ â”œâ”€â”€ silver/               # Raw â†’ Cleaned transformations
â”‚ â””â”€â”€ gold/                 # Business-ready datasets
â”œâ”€â”€ rawdata/                # DataSource
â”œâ”€â”€ docs/                   # Architecture diagrams & documentation
â””â”€â”€ scripts/                # Connection/utility scripts
```


## ğŸš€ Pipeline Architecture
```
â–Œ Extraction Phase
â”œâ”€ Azure Data Factory copies from GitHub â†’ Azure SQL DB
â””â”€ Initial schema validation applied

â–Œ Bronze Layer (Raw)
â”œâ”€ ADF lands data in Azure Storage (Parquet)
â””â”€ Partitioned by ingestion date

â–Œ Silver Layer (Cleaned)
â”œâ”€ Databricks notebooks transform raw data:
â”‚  â”œâ”€ Reading and Schema validation
â”‚  â”œâ”€ Transformation
â”‚  â””â”€ Data Writing
â””â”€ Output: Validated Parquet files

â–Œ Gold Layer (Business Ready)
â”œâ”€ Delta Lake tables with SCD Type 1 logic
â”œâ”€ Star schema implementation:
â”‚  â”œâ”€ Fact tables (metrics)
â”‚  â””â”€ Dimension tables (context)
â””â”€ Optimized for Power BI queries

â–Œ Visualization
â””â”€ Power BI DirectQuery to Delta tables
```

## Key Features

### Medallion Architecture Implementation
- Bronze: Immutable raw data layer
- Silver: Validated business entities
- Gold: Optimized analytics tables

### Slowly Changing Dimensions (Type 1) Implementation
```
deltatable.alias('t').merge(df_fact.alias('s'),\
        t.dim_branch_key == s.dim_branch_key and \
        t.dim_dealer_key == s.dim_dealer_key and \
        t.dim_model_key == s.dim_model_key and \
        t.dim_date_key == s.dim_date_key)\
                .whenMatchedUpdateAll()\
                .whenNotMatchedInsertAll()\
                .execute()
```


## Azure DataBrics Model Execution
![Databricks](docs/Azure-Databrics.png "Databrics pipeline")


## Azure DataFactory Pipeline
![Datafactory](docs/Azure-DataFactory.png "Datafactory pipeline")



## ğŸ“Š Sample Output

### Gold Layer Data Model
Star Schema

### Power BI Dashboard
Visualization

## ğŸ› ï¸ Setup Instructions
Prerequisites
- Azure subscription
- Contributor access to:
  - Data Factory
  - Databricks workspace
  - Storage Account


## ğŸ¤ Contributing
- Fork the project
- Create your feature branch (git checkout -b feature/improvement)
- Commit changes (git commit -m 'Add new transformation')
- Push to branch (git push origin feature/improvement)
- Open a Pull Request